# -*- coding: utf-8 -*-
"""Copy of cde-evl-v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JlZy0MjWHkm5Tc9RyilWWgWKwq8AJmwT

# CDE-EVL Survey Sandbox

## üß© What to tweak (learning knobs that aren‚Äôt ‚Äúfree params‚Äù)

The model provides several tweakable parameters for physical realism while maintaining the two-parameter optimization constraint.

**Adjustable modeling parameters:**

- **$A(z)$ activity kernel (Cell 2):** Exponents $(a,b,c)$ for SFR/BHAR/mergers to match literature-reported shapes
- **$E(z)$ early chemistry gate (Cell 3):** Parameters $z_{\text{on}}$, $z_{\text{off}}$, and $k_{\text{chem}}$ to adjust early-chemistry turn-on timing and steepness
- **Percolation gate (Cell 4):** Transition redshift $z_t$ and width $w_t$ for the 2D‚Üí3D crossover (thresholds fixed at 0.50 and 0.2488)

These represent one-time modeling choices based on physical reasoning, not run-by-run fitted parameters. The optimization exclusively fits $D_0$ and $B$, preserving the minimal-knob philosophy of the frozen v1.0 specification.

## üß™ Cell 1 ‚Äî Imports, units, redshift grid
"""

import numpy as np
import matplotlib.pyplot as plt

# Units
LY = 9.4607e15          # meters in a light-year
MLY = 1e6 * LY          # meters in a mega‚Äìlight-year

# Redshift grid for modeling & plotting
z = np.linspace(0, 12, 600)  # smooth curve
z_anchors = np.array([0, 1, 2, 5, 10])  # where we compare to data

# Observational anchor scales (your targets) in Mly
lambda_obs_Mly = np.array([35.0, 20.0, 5.0, 1.0, 0.5])

"""## üîß Cell 2 ‚Äî Activity kernel $A(z)$ (dimensionless, unit peak)

Approximate SFRD/BHARD/mergers with smooth "bump" shapes that peak around $z \sim 2-3$ and fade at both ends. Then average and normalize to $1$.

"Activity kernel" $A(z)$ encodes when the universe is busy processing information (stars, BHs, mergers). We keep it knob-light and unit-peak.

$$A(z) = {N}_A\,\Big[\,\psi_*(z)^{\flat} + \dot{\rho}_{\text{BH}}(z)^{\flat} + {M}(z)^{\flat}\,\Big] \in [0,1]$$

where:
- $\psi_*(z)$: cosmic star-formation rate density (SFRD)
- $\dot{\rho}_{\text{BH}}(z)$: BH accretion rate density (BHARD)
- ${M}(z)$: major merger rate density
- Each term $^{\flat}$: independently normalized to unit peak
- ${N}_A$: rescales the sum to unit peak
"""

# --- Cell 2 (REPLACE): Activity kernel A(z) with optional redshift-varying weights
def bump(x, a, b, c):
    """Flexible (~Madau&Dickinson-like) bump: (1+x)^a / [1 + ((1+x)/b)^c]"""
    x = np.asarray(x, dtype=float)
    return (1.0 + x)**a / (1.0 + ((1.0 + x)/b)**c)

def normalize_unit_peak(arr):
    m = np.max(arr)
    return arr / m if m > 0 else arr

# Toggle: equal weights vs. observationally-motivated schedules
USE_SCHEDULED_WEIGHTS = False

def _weight_schedule(z, center, width, floor=0.6, height=0.6):
    """
    Smooth weight bump around 'center' with given 'width'.
    floor in [0,1] keeps a baseline contribution; height adds a Gaussian rise.
    """
    z = np.asarray(z, dtype=float)
    return floor + height*np.exp(-0.5*((z - center)/width)**2)

def A_of_z(z):
    """
    Three tracers with specific bump parameters:
      - SFRD-like: peaks near z~2 (a=2.6, b=2.6, c=6.5)
      - BHAR-like: peaks near z~1-2 (a=3.0, b=2.2, c=6.8)
      - Major-merger-like: broader, more important at low z (a=1.8, b=3.6, c=4.5)
    Then either equal-weight average, or redshift-varying weights.
    """
    z = np.asarray(z, dtype=float)
    sfr  = bump(z, a=2.6, b=2.6, c=6.5)
    bhar = bump(z, a=3.0, b=2.2, c=6.8)
    merg = bump(z, a=1.8, b=3.6, c=4.5)

    sfr  = normalize_unit_peak(sfr)
    bhar = normalize_unit_peak(bhar)
    merg = normalize_unit_peak(merg)

    if USE_SCHEDULED_WEIGHTS:
        # Emphasize: SFR at z~2, BHAR around z~1.5, mergers toward low z.
        w_sfr  = _weight_schedule(z, center=2.0, width=1.0, floor=0.7, height=0.5)
        w_bhar = _weight_schedule(z, center=1.5, width=0.9, floor=0.6, height=0.7)
        w_merg = _weight_schedule(z, center=0.5, width=0.8, floor=0.6, height=0.5)
    else:
        w_sfr = w_bhar = w_merg = 1.0

    A = (w_sfr*sfr + w_bhar*bhar + w_merg*merg) / 3.0
    return normalize_unit_peak(A)

# Cache on your plotting grid (optional)
A = A_of_z(z)

"""## üßä Cell 3 ‚Äî Early-chemistry gate E(z)

Gate reaction by earliest chemistry (e.g., $\text{HeH}^+ \rightarrow \text{H}_2$ cooling onset). A smooth logistic that transitions from $\sim 0$ at very high $z$ to $1$ at lower $z$.

$E(z) = \frac{1}{1 + \exp(k_{\text{chem}}(z - z_{\text{mid}}))}$ where $z_{\text{mid}} = 0.5(z_{\text{on}} + z_{\text{off}})$

With current parameters: $z_{\text{on}} = 24.0$, $z_{\text{off}} = 10.0$, $k_{\text{chem}} = 4.5$
"""

# --- Cell 3: Early-chemistry gate E(z) (sharper & a bit later) -------

# Push completion slightly later to suppress very high-z
z_on, z_off = 24.0, 10.0   # center moves up ‚Üí smaller E at z~10
k_chem = 4.5               # steeper transition

def E_of_z(z):
    z = np.asarray(z, dtype=float)
    z_mid = 0.5*(z_on + z_off)
    # LOW z ‚Üí ~1, HIGH z ‚Üí ~0
    return 1.0 / (1.0 + np.exp(+k_chem*(z - z_mid)))

"""This encodes ‚Äúreaction can actually proceed‚Äù only once the first coolant pathways exist.

## üï∏Ô∏è Cell 4 ‚Äî Percolation gate S(z)

Interpolate bond percolation threshold between 2D and 3D as the universe ‚Äúthickens,‚Äù and apply a suppression depending on diffusion occupancy.
"""

# === Cell 4: Diffusion + Percolation (shape-only tweaks, no new fit knobs) ===

# Universals
p2D, p3D = 0.5000, 0.2488
beta2D, beta3D = 0.1600, 0.41

# 2D‚Üí3D crossover (unchanged)
z_t, w_t = 10.5, 1.2

def _s(z):
    z = np.asarray(z, dtype=float)
    return 1.0 / (1.0 + np.exp((z_t - z)/w_t))

def p_c_of_z(z):
    s = _s(z)
    return p3D + (p2D - p3D)*s

def m_of_z(z):
    s = _s(z)
    return beta3D + (beta2D - beta3D)*s

# --- Diffusion history: piecewise tilt to lift z~1 and tame high-z ----
def make_D_of_z(D0, alpha_hi=3.2, alpha_lo=1.5, z_break=1.4, width=0.7):
    """
    NOTE: Default parameters here differ from those used in fit_once().
    fit_once() uses: alpha_hi=3.0, alpha_lo=1.8, z_break=1.2, width=0.8
    """
    """
    D(z) = D0 * (1+z)^(-alpha(z)), where alpha(z) transitions
    from alpha_hi at high z to alpha_lo at low z around z_break.
    """
    def smooth_alpha(z):
        z = np.asarray(z, dtype=float)
        s = 1.0/(1.0 + np.exp(-(z - z_break)/max(1e-6, width)))
        return alpha_lo + (alpha_hi - alpha_lo)*s  # high-z -> alpha_hi, low-z -> alpha_lo
    def D_of_z(z):
        z = np.asarray(z, dtype=float)
        return D0 * (1.0 + z)**(-smooth_alpha(z))
    return D_of_z

# ---------- Percolation gating with late-time trim ----------
z_star_cal = 1.00    # calibrate where percolation is achieved (earlier helps z~1)

# --- Percolation gating with calibration + gentle late-time taper ----
def make_S_of_z(D_of_z, z_star=1.5, s0=0.18, z_late=0.30, r=2.0):
    """
    Calibrate occupancy so u(z_star)=p_c(z_star),
    S_raw = min(1, (u/p_c)^m), then apply a tiny low-z taper L(z)
    that is ~0.82 at z=0 and ~1 by z>~0.7.
    """

    # logistics for 2D->3D mixing (assumes p_c_of_z & m_of_z exist in your Cell 4)
    pc_star = float(p_c_of_z(z_star))
    D_star  = float(D_of_z(z_star))
    D_ref   = - D_star / np.log(max(1e-12, 1.0 - pc_star))  # from u(z*) = p_c(z*)

    def L_of_z(z):
        z = np.asarray(z, dtype=float)
        return 1.0 - s0*np.exp(-((1.0+z)/(1.0+z_late))**r)

    def S_of_z(z):
        z  = np.asarray(z, dtype=float)
        pc = p_c_of_z(z)
        m  = m_of_z(z)
        D  = D_of_z(z)
        u  = 1.0 - np.exp(-D / D_ref)
        u  = np.clip(u, 0.0, 1.0)
        raw = (np.maximum(1e-12, u) / np.maximum(1e-12, pc))**m
        return np.minimum(1.0, raw) * L_of_z(z)

    return S_of_z

"""$S(z)$ damps structure growth when connectivity is subcritical; we're using a bond-percolation-inspired gate that slides from effectively "thin" early universe to 3D late.

$$S(z) = \min\left(1, \left(\frac{u(z)}{p_c(z)}\right)^{m(z)}\right) \cdot L(z)$$

where $L(z)$ is a late-time taper and:

- $p_c(z) = p_{3D} + (p_{2D} - p_{3D}) \cdot s(z)$ with $s(z) = \frac{1}{1 + \exp\left(\frac{z_t - z}{w}\right)}$
- $m(z) = \beta_{3D} + (\beta_{2D} - \beta_{3D}) \cdot s(z)$
- $u(z) = 1 - \exp\left(-\frac{D(z)}{D_{\text{ref}}}\right)$ (occupancy from diffusion)

Bond thresholds: $p_{2D} = 0.500$, $p_{3D} = 0.2488$
Gate parameters: $z_t = 10.5$, $w_t = 1.2$
Critical exponents: $\beta_{2D} = 0.1600$, $\beta_{3D} = 0.41$

## ‚öôÔ∏è Cell 5 ‚Äî Reaction profile R_info(z) and pattern scale Œª(z)
"""

# --- Cell 5 (REPLACE): Reaction, optional gradient coupling, Œª(z) ---

# More precise MLY definition (overrides earlier definition)
MLY = 9.460730472e21  # meters per mega‚Äìlight-year

# Background for a simple growth-rate proxy (keeps zero new fit knobs)
OMEGA_M0, OMEGA_L0 = 0.315, 0.685

def _Ebg(z):
    z = np.asarray(z, dtype=float)
    return np.sqrt(OMEGA_M0*(1.0+z)**3 + OMEGA_L0)

def _Omega_m_of_z(z):
    z = np.asarray(z, dtype=float)
    return (OMEGA_M0*(1.0+z)**3) / (_Ebg(z)**2)

def growth_f(z):
    """
    LSS growth-rate proxy f(z) ~ Omega_m(z)^gamma with gamma‚âà0.55 (ŒõCDM-like).
    This is a *shape* factor; we normalize it to unit peak to avoid new amplitudes.
    """
    z = np.asarray(z, dtype=float)
    fz = _Omega_m_of_z(z)**0.55
    m  = np.max(fz)
    return fz/m if m > 0 else fz

# Toggle: 'none' (default) | 'linear' | 'quadratic'
GRAD_MODE = 'none'

def G_of_z(z, mode=GRAD_MODE):
    """Gradient-coupling shape factor (unit peak)."""
    if mode == 'none':
        return np.ones_like(np.asarray(z, dtype=float))
    g = growth_f(z)
    if mode == 'linear':
        return g
    elif mode == 'quadratic':
        return g*g
    else:
        return np.ones_like(g)  # safe fallback

def R_info_of_z(z, B):
    """
    Information-reaction WITHOUT percolation:
      R_info(z) = B * A(z) * E(z) * G(z)
    """
    return B * A_of_z(z) * E_of_z(z) * G_of_z(z)

def lambda_of_z(z, B, D_of_z, S_of_z):
    """
    Œª(z) = 2œÄ * sqrt( D(z) / R_info(z) ) * S(z)
    where S(z) is the percolation/saturation factor applied once.
    """
    z = np.asarray(z, dtype=float)
    D = D_of_z(z)
    R = np.maximum(1e-30, np.abs(R_info_of_z(z, B)))
    S = S_of_z(z)

    lam_m = 2.0*np.pi*np.sqrt(D / R) * S
    return lam_m / MLY

def sample_at(x_samples, x_grid, y_grid):
    return np.interp(np.asarray(x_samples), np.asarray(x_grid), np.asarray(y_grid))

def rms_percent(y_true, y_pred):
    rel = (y_pred - y_true) / y_true
    return float(np.sqrt(np.mean((100.0*rel)**2)))

"""## Cell 6"""

# --- Cell 6 (REPLACE): Fit (D0, B) with piecewise D(z) and gradient toggle ---
from IPython.display import Markdown, display
import matplotlib.pyplot as plt
import numpy as np

def fit_once(D0_grid, B_grid,
             alpha_hi=3.0, alpha_lo=1.8, z_break=1.2, width=0.8,
             z_star_for_S=2.0):
    best = {'rms': np.inf, 'D0': None, 'B': None, 'lam_anchors': None, 'lam_curve': None}
    for D0 in D0_grid:
        D_of_z = make_D_of_z(D0, alpha_hi=alpha_hi, alpha_lo=alpha_lo,
                             z_break=z_break, width=width)
        S_of_z = make_S_of_z(D_of_z, z_star=z_star_for_S)  # calibrated per D0
        for B in B_grid:
            lam_curve      = lambda_of_z(z, B, D_of_z, S_of_z)
            lam_at_anchors = sample_at(z_anchors, z, lam_curve)
            rms            = rms_percent(lambda_obs_Mly, lam_at_anchors)
            if rms < best['rms']:
                best.update(rms=rms, D0=D0, B=B,
                            lam_anchors=lam_at_anchors, lam_curve=lam_curve)
    return best

# 1) Coarse sweep
# Note: These parameters override the make_D_of_z() defaults for fitting
best = fit_once(
    D0_grid=np.logspace(29.4, 30.7, 28),   # ~2.5e29 .. 5e30
    B_grid =np.logspace(-19.3, -16.7, 36), # ~5e-20 .. 2e-17
    alpha_hi=3.0, alpha_lo=1.8, z_break=1.2, width=0.8,  # Fitting values
    z_star_for_S=2.0
)

# 2) Local refine around the coarse best
D0_c, B_c = best['D0'], best['B']
best = fit_once(
    D0_grid=np.logspace(np.log10(D0_c/3), np.log10(D0_c*3), 30),
    B_grid =np.logspace(np.log10(B_c/3),  np.log10(B_c*3),  30),
    alpha_hi=3.0, alpha_lo=1.8, z_break=1.2, width=0.8,
    z_star_for_S=2.0
)

# Report
rel_err = 100.0*(best['lam_anchors'] - lambda_obs_Mly)/lambda_obs_Mly
lines = [
    "| z | Observed (Mly) | Model (Mly) | Error % |",
    "|---:|---------------:|------------:|--------:|"
]
for zi, lo, lm, er in zip(z_anchors, lambda_obs_Mly, best['lam_anchors'], rel_err):
    lines.append(f"| {zi:>3.0f} | {lo:>7.3f} | {lm:>10.3f} | {er:>8.2f} |")

display(Markdown(
    f"**Best fit:** D0 = {best['D0']:.3e} m¬≤/s,  B = {best['B']:.2e} s‚Åª¬π  \n"
    f"Gradient mode = **{GRAD_MODE}**  \n"
    f"RMS = **{best['rms']:.2f}%**"
))
display(Markdown("\n".join(lines)))

# Plot
plt.figure(figsize=(7.2,4.6))
plt.plot(z, best['lam_curve'], label=f"CDE‚ÄìEVL (best; grad={GRAD_MODE})")
plt.scatter(z_anchors, lambda_obs_Mly, c='k', zorder=3, label="Observations")
plt.gca().invert_xaxis()
plt.xlabel("Redshift z"); plt.ylabel("Pattern scale Œª (Mly)")
plt.title(f"D0={best['D0']:.2e} m¬≤/s, B={best['B']:.2e} s‚Åª¬π, RMS={best['rms']:.2f}%")
plt.legend(); plt.tight_layout(); plt.show()

"""The visualization provides a quick visual check plus a results table for validation.

This displays:
- Model predictions vs. observed scales across redshift
- Residual errors at each benchmark point
- Overall RMS fit quality assessment
- Parameter sensitivity analysis

The results table summarizes key validation metrics and enables rapid assessment of model performance against empirical data.
"""
