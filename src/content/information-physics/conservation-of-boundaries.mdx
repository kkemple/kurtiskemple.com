---
layout: "../../layouts/BlogPost.astro"
title: "Conservation of Boundaries"
description: "All system change may occur through three operations: MOVE, JOIN, SEPARATE. Explore the proposed law governing how boundaries transform and why the same mechanics that build can destroy."
image: "/images/og/conservation-of-boundaries.png"
pubDate: "07/20/2025"
---

All system transformation—whether building a startup, causing a revolution, or designing software—follows the same mechanical principles. Change occurs through exactly three operations, no more, no less. This isn't a framework or model. It's how system entropy actually changes.

The Conservation of Boundaries (COB) states that boundaries cannot be created or destroyed, only transformed. Like conservation of energy in physics, this law governs how entropy changes in all human systems. Whether the goal is innovation or collapse, improvement or degradation, the mechanics remain constant.

> **Conservation of Boundaries (COB):** A foundational law stating that all system transformation—whether entropy-increasing or entropy-reducing—occurs through one of three irreducible operations: **MOVE**, **JOIN**, or **SEPARATE**.
>
> These operations are applied to existing boundaries within a system, whether between people, information, roles, or structures. No fourth operation has been observed. All meaningful change decomposes to one or more of these primitives.

Understanding COB changes system change from art to engineering. The same operations that reduce entropy in the internet can increase it in a company. The same mechanics that enable major innovation can cause catastrophic failure. The direction depends entirely on how these operations are applied.

---

## The Universal Formula

System entropy change follows a precise mathematical relationship that accounts for both the operations performed and the resistance encountered. This formula applies whether optimizing a team of five or changing a civilization of millions.

> **System Entropy Change (SEC):** The measurable impact a conscious agent can have on system entropy from their specific position, calculated through observer-dependent mathematics where position, intent, and operations determine possibility.
>
> `SEC = O × V / (1 + E)`

Where each variable captures a core aspect of system change:

- **O** = Operations performed (MOVE, JOIN, SEPARATE)
- **V** = Vector of actor-group conscious intent (positive for entropy reduction, negative for entropy increase)
- **E** = Entropy as measured from individual actor's position (lived reality/informational constraints/entropy from the system)

The critical insight is that E varies by position in the system. A CEO experiences different entropy than a front-line worker when implementing the same change. This observer-dependent reality explains why identical operations produce different results from different positions—a phenomenon traditional change models fail to capture.

![The illustration shows the equation SEC = O × V / (1 + E).](/images/blog/first-entropic-equation.png)

For a practical demonstration of the SEC equation in action with real numbers, see [Cultural Percolation: When Language Reaches Critical Mass](/information-physics/entropic-mathematics#cultural-percolation-when-language-reaches-critical-mass)—showing how different cultural groups (E = 0.20, 0.35, 0.60) achieve dramatically different outcomes when adopting new language like "rizz."

---

## The Three Operations

Every transformation in human history reduces to these three operations. They cannot be decomposed further, and no fourth operation exists. Understanding their pure forms reveals why certain changes succeed while others fail.

### MOVE

The MOVE operation shifts boundaries to new positions or contexts while preserving their essential structure. This is the most common operation because it requires the least energy—you're not creating or destroying connections, just repositioning them.

Historical MOVE operations that changed civilization:

- **Printing press (1440)**: Moved knowledge reproduction from monasteries to everywhere
- **Containerization (1956)**: Moved cargo handling from dockside to origin/destination
- **ATMs (1967)**: Moved banking from inside banks to street corners
- **Cloud computing (2006)**: Moved computation from local machines to distributed centers

Each MOVE operation solved a boundary constraint by repositioning where work happens. The pattern is consistent: identify where current boundaries create friction, then move them to where they flow naturally. The vector—intent to improve versus intent to disrupt—determines whether the move reduces or increases system entropy.

### JOIN

The JOIN operation combines previously separate boundaries into unified wholes. This creates new capabilities through combination but requires overcoming the entropy of keeping things separate.

Major JOIN operations throughout history:

- **TCP/IP (1974)**: Joined isolated computer networks into the internet
- **European Union (1993)**: Joined national economies into single market
- **Disney + Pixar (2006)**: Joined traditional and computer animation expertise
- **Mobile OS ecosystems (2008)**: Joined phones, apps, and services into platforms

JOIN operations succeed when the combined entity achieves capabilities neither component could alone. They fail when forced combinations create more entropy than they resolve. The vector—the intent behind the joining—determines whether synergy or chaos results.

### SEPARATE

The SEPARATE operation divides unified boundaries into distinct parts. This enables focused optimization but requires accepting the overhead of coordination.

Historic SEPARATE operations in history:

- **American Revolution (1776)**: Separated colonies from empire
- **Bell System breakup (1984)**: Separated telecom monopoly into competing entities
- **iTunes Store (2003)**: Separated songs from albums
- **Microservices (2014)**: Separated monolithic applications into discrete services

SEPARATE operations work when artificial boundary coupling creates more problems than authentic connection. They fail when natural dependencies are severed, creating coordination overhead that exceeds efficiency gains. The intent—reduce entropy versus increase entropy—shapes the outcome.

---

## Symmetry and Reversal

**COB** operates symmetrically—the same operations that reduce entropy can increase it. **Vector** determines direction, *but the mechanics remain identical.* This symmetry explains both innovation and collapse through a single framework.

Consider how the same operations reverse with negative vector:

- **MOVE** becomes dislocation when intent is to disrupt
- **JOIN** becomes toxic coupling when forcing incompatible elements
- **SEPARATE** becomes harmful fragmentation when breaking natural bonds

The formula captures this through the V variable. Positive V reduces system entropy, negative V increases it. A company can be destroyed as systematically as it was built, using the exact same operations with opposite vector.

---

## Why COB Works

Conservation of Boundaries works because it describes mechanics, not intentions. Just as gravity affects all objects regardless of their purpose, COB governs all system entropy changes regardless of their goals.

The law emerges from three fundamental constraints:

1. **Cognitive limits**: Humans can only conceptualize three basic spatial operations
2. **Physical reality**: Objects can only be moved, combined, or divided
3. **Information structure**: Data relationships follow the same three patterns

Every complex entropy change decomposes to these primitives. A corporate merger might involve thousands of decisions, but each one is ultimately a MOVE, JOIN, or SEPARATE operation applied to some boundary in the system.

---

## Observer-Dependent Entropy

The E variable in the formula captures why the same change feels easy from one position and impossible from another. This isn't perception—it's measurable reality based on position in the system.

A practical example demonstrates this clearly. When a company implements new software:

**From executive position (E = 0.2)**:

- Sign purchase order
- Announce at company meeting
- View dashboards of adoption metrics
- Experience: "This is straightforward"

**From manager position (E = 0.6)**:

- Coordinate multiple team schedules
- Handle resistance from reports
- Translate between executive vision and team reality
- Experience: "This is challenging but manageable"

**From worker position (E = 0.9)**:

- Learn new system while maintaining productivity
- No control over timeline or training
- Deal with bugs and missing features
- Experience: "This is nearly impossible"

Same change, same operations, wildly different entropy based on position. COB captures this reality mathematically, explaining why changes that seem simple from the top become exponentially harder as they cascade through organizations.

---

## Implications

Conservation of Boundaries suggests several important truths about how system entropy changes. These insights change how we approach change at every scale.

First, there are no unique entropy changes—only unique combinations of the three operations. This means any successful change can be studied, decomposed, and potentially replicated by understanding its constituent operations.

Second, position matters more than plan. The most sophisticated strategy will fail if executed from a position of high entropy. Conversely, simple operations from low-entropy positions can change entire systems.

Third, resistance isn't uniform—it's observer-dependent. What feels like organizational resistance might simply be positional entropy. Solving for E often matters more than perfecting O or V.

Finally, all systems change entropy through the same mechanics. The operations that organize a desk work identically on global economies. Only scale and complexity change—the fundamental operations remain constant.

Conservation of Boundaries isn't a framework to adopt or methodology to implement. It's the mechanical reality of how entropy changes in human systems. Understanding it is like understanding gravity—useful whether you believe in it or not.

> - [**Information Physics Field Guide**](/information-physics/field-guide): The field guide to Information Physics.
> - [**Information Physics LLM Friendly Study Guide**](/information-physics-study-guide.txt): Drop this in your context and ask AI to explain Information Physics objectively.
> - [**Information Physics**](/information-physics/theory): A general theory describing how conscious beings reduce or increase entropy through three operations on information, coordination, and system boundaries.
> - [**Entropic Mathematics**](/information-physics/entropic-mathematics): A proposed applied field of mathematics extending established tools (Shannon entropy, vector calculus, information theory) to conscious systems where observer position and lived experience may be fundamental calculation variables.
> - [**Entropic Gap**](/information-physics/entropic-gap): A framework that may help detect system decay before it becomes catastrophic by calculating the distance between intended and current states.
> - [**Entropic Equilibrium**](/information-physics/entropic-equilibrium): A theory exploring why systems may stabilize where they do through observer-dependent optimization.
> - [**Information Physics Throughout History**](/information-physics/throughout-history): How Sun Tzu, Machiavelli, and Napoleon may have intuitively applied IP principles centuries before the mathematics existed.
> - [**Information Physics In Mathematics**](/information-physics/in-mathematics): Exploring how established mathematics (Shannon entropy, vector calculus, information theory) might extend into conscious systems where observer position and lived experience become fundamental variables rather than complications to eliminate.
> - [**Information Physics In Science**](/information-physics/in-science): How IP may reveal the underlying principle that unites quantum mechanics, biology, and cosmology across all scales.
> - [**Renaissance Florence vs Silicon Valley: The Innovation Entropy Crisis**](/information-physics/the-innovation-entropy-crisis): Comparing how Silicon Valley may produce 12x fewer innovators per capita than Renaissance Florence despite vastly superior resources—suggesting technology cannot overcome high entropy.
> - [**Constraint by Design: Entropy Limits in the Gig Economy**](/information-physics/entropy-limits-in-gig-economy): Mathematical analysis suggesting that gig economy architecture may make worker advancement impossible regardless of individual effort, potentially demonstrating how structural position determines capability.
> - [**Survival Trends Across Mass Extinctions**](/information-physics/survival-trends-in-mass-extinction-events): The fossil record suggests a pattern: during mass extinction events, specialists died while generalists thrived. This pattern may represent Information Physics playing out at planetary scale.
> - [**The Peasant**](/the-peasant.txt): A playbook for creating positive-sum outcomes in high-entropy (negative-sum) environments.
> - [**The "Just How It Is" Test**](/information-physics/just-how-it-is-test): Test Information Physics against traditional frameworks on any stubborn "unchangeable" problem to see which approach may work better from your position.
